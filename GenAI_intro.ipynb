{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Geneartive AI encompasses algorithms that can produce new content across various media , such as text , images , music , and more. This groundbreaking technology is transforming multiple industries by facilitating innovative creativity and enhancing automation processes."
      ],
      "metadata": {
        "id": "gEMgMgzGYIAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KSA7eWwbsqYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Application of GenAI\n",
        "\n",
        "It can be used in diverse fields such as art music.\n",
        "\n",
        "Technology behind it -\n",
        "\n",
        "It often uses techniques like NNmand DL to generate outputs that mimic human like creativity.\n",
        "\n",
        "TRADITIONAL AI VS GENAI -\n",
        "\n",
        "Traditional AI : Analyzesndata for taska like classification and prediction , relying on pre-existing dataset.\n",
        "\n",
        "Generative AI : Focuses on creating content generating unique outputs beyond the new data .\n",
        "Use\n",
        "\n",
        "Use Cases :\n",
        "\n",
        "Traditional AI in recommendation system; Generative AI gor imgs , text , and music.\n",
        "\n",
        "GENAI compared to ML , DL ,NLP\n",
        "\n",
        "ML : A broader field that encompasses algorithms that learn from the data , which may or may not involve generative tasks.\n",
        "\n",
        "DL : A subset of ML that uses neural networks with many layers . Generative AI often employs DL models to produce high-quality outputs.\n",
        "\n",
        "NLP : A field focuses on the introduction between coomputers and human language , where GENAI can create human like text responses.\n",
        "\n",
        "Real Use Cases of GenAI\n",
        "\n",
        "1. Content Creation : Tools like chatgpr DALL-E\n",
        "2. Gaming : Used to create dynamic game environment and character dialogues , enhancing player experience .\n",
        "3. HealthCare : It can assist in drug discovery by generating molecular structure or data.\n",
        "\n",
        "GenAI Models:\n",
        "\n",
        "1. GPT : A language model designed for text generation.\n",
        "2. DALL-E : Used for image generation.\n",
        "3. StyleGUN : used for generating high quality imgs.\n",
        "\n",
        "Ethical Challanges :\n",
        "\n",
        "1. Bias and fairness : GenAI can perpetuate biases present in training data , leading to unfair outputs.\n",
        "2. Misinformation : The ability to create hyper-realistic content raises concerns about fake news .\n",
        "3. Intellectual Property : Questions about ownership arise"
      ],
      "metadata": {
        "id": "PXuEU-4hY_aG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auto Encoder are neural network that aim to learn accompressed , lower dimensions representation of the input data.\n",
        "\n",
        "They consist of an encoder and the decoder.\n",
        "\n",
        "The encoders maps the input dagta to a latent space and the dedcoder reoncept inpit from this latent representation.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gVF_QDEUkedL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veriational AUtoencoder ğŸ‡°\n",
        "\n",
        "It extend autoencoders by encoding the input data into a probability distribution in the latent space.\n",
        "\n",
        "The latent space is a lower dimensional representation of the input data learned by autoencoder.\n",
        "\n",
        "Mean + Variance = Probability Distribution"
      ],
      "metadata": {
        "id": "WjGPPFrhl44r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autoencoder workflow -\n",
        "1. Input text (TF - IDF)\n",
        "2. Encoder (Dense layer -> Vector)\n",
        "3. Decoder (Vector -> Dense Layer)\n",
        "4. Reconstructed (TF-IDF)"
      ],
      "metadata": {
        "id": "18T8UncLstpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "varitional autoencoder\n",
        "1. Input text (TF-IDF)\n",
        "2. Encoder (Mean = epsilon - [mean,variance(ar)])\n",
        "3. Reparamaterization trick : [z = mean + variance * epsilon (epsilon -> N(0,1)]\n",
        "4. Decoder (Dense layer -> Vector)\n",
        "5. Top terms != original text"
      ],
      "metadata": {
        "id": "Y3TmY08dtSrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"AI is Transforming the world.\", \"DL model requires large data sets.\", \"NLP enable chatbots.\"\n",
        ",\"Computer vision helps in img recommendation\" , \"Generative models create realistic content.\"]"
      ],
      "metadata": {
        "id": "5FQ0IeNzuzZ_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF Vectorization\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(text).toarray()\n",
        "vocab =vectorizer.get_feature_names_out()\n",
        "in_dim = X.shape[1]\n",
        "print(vocab)\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFdRaSiivrWr",
        "outputId": "013eb76d-089d-4e4e-ec7f-a6453c79967d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ai' 'chatbots' 'computer' 'content' 'create' 'data' 'dl' 'enable'\n",
            " 'generative' 'helps' 'img' 'in' 'is' 'large' 'model' 'models' 'nlp'\n",
            " 'realistic' 'recommendation' 'requires' 'sets' 'the' 'transforming'\n",
            " 'vision' 'world']\n",
            "[[0.4472136  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.4472136  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.4472136  0.4472136  0.\n",
            "  0.4472136 ]\n",
            " [0.         0.         0.         0.         0.         0.40824829\n",
            "  0.40824829 0.         0.         0.         0.         0.\n",
            "  0.         0.40824829 0.40824829 0.         0.         0.\n",
            "  0.         0.40824829 0.40824829 0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.57735027 0.         0.         0.         0.\n",
            "  0.         0.57735027 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.57735027 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.40824829 0.         0.         0.\n",
            "  0.         0.         0.         0.40824829 0.40824829 0.40824829\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.40824829 0.         0.         0.         0.         0.40824829\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.4472136  0.4472136  0.\n",
            "  0.         0.         0.4472136  0.         0.         0.\n",
            "  0.         0.         0.         0.4472136  0.         0.4472136\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input , Dense\n",
        "en_dim=5\n",
        "input_layer = Input(shape=(in_dim,))\n",
        "encoded_layer = Dense(en_dim,activation = \"relu\")(input_layer)\n",
        "decoded_layer = Dense(in_dim,activation = \"sigmoid\")(encoded_layer)\n",
        "autoencoder = Model(input_layer,decoded_layer)\n",
        "autoencoder.compile(optimizer=\"adam\",loss=\"binary_crossentropy\")\n",
        "autoencoder.fit(X,X,epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGtMCsofwnLV",
        "outputId": "7ef6c931-5a90-449f-9dd0-40dc741bc4f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 0.6950\n",
            "Epoch 2/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.6941\n",
            "Epoch 3/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.6932\n",
            "Epoch 4/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.6923\n",
            "Epoch 5/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.6914\n",
            "Epoch 6/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.6905\n",
            "Epoch 7/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.6896\n",
            "Epoch 8/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.6887\n",
            "Epoch 9/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.6878\n",
            "Epoch 10/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.6870\n",
            "Epoch 11/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 0.6861\n",
            "Epoch 12/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.6852\n",
            "Epoch 13/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.6844\n",
            "Epoch 14/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.6835\n",
            "Epoch 15/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 0.6826\n",
            "Epoch 16/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.6818\n",
            "Epoch 17/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.6809\n",
            "Epoch 18/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.6801\n",
            "Epoch 19/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.6793\n",
            "Epoch 20/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.6784\n",
            "Epoch 21/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.6776\n",
            "Epoch 22/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.6768\n",
            "Epoch 23/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 0.6759\n",
            "Epoch 24/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.6751\n",
            "Epoch 25/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - loss: 0.6743\n",
            "Epoch 26/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.6734\n",
            "Epoch 27/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.6726\n",
            "Epoch 28/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.6718\n",
            "Epoch 29/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.6709\n",
            "Epoch 30/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.6701\n",
            "Epoch 31/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.6693\n",
            "Epoch 32/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.6685\n",
            "Epoch 33/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.6676\n",
            "Epoch 34/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.6668\n",
            "Epoch 35/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.6660\n",
            "Epoch 36/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.6651\n",
            "Epoch 37/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.6643\n",
            "Epoch 38/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.6635\n",
            "Epoch 39/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.6626\n",
            "Epoch 40/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.6618\n",
            "Epoch 41/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.6610\n",
            "Epoch 42/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.6601\n",
            "Epoch 43/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.6593\n",
            "Epoch 44/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.6584\n",
            "Epoch 45/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.6576\n",
            "Epoch 46/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.6567\n",
            "Epoch 47/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.6559\n",
            "Epoch 48/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.6550\n",
            "Epoch 49/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.6541\n",
            "Epoch 50/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 0.6533\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7884a4f6cd90>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auto_output = autoencoder.predict(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRbW7EbM24N7",
        "outputId": "91a1ea20-87ca-428b-c27b-3f58da7b5ac4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auto_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVdPfRUj3BoC",
        "outputId": "1149d58d-605f-472a-9ea4-e7796d4b9d94"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.50679505, 0.49430162, 0.45474514, 0.46136197, 0.46993926,\n",
              "        0.44602734, 0.5045979 , 0.44246513, 0.47337377, 0.48913738,\n",
              "        0.4340985 , 0.49652988, 0.5251709 , 0.46406582, 0.5064138 ,\n",
              "        0.45249796, 0.44037485, 0.46909863, 0.4604547 , 0.46300417,\n",
              "        0.4549587 , 0.4502145 , 0.488635  , 0.46497995, 0.5169477 ],\n",
              "       [0.51879305, 0.48432386, 0.45417193, 0.44385812, 0.4567552 ,\n",
              "        0.4320627 , 0.5109916 , 0.4289296 , 0.43797892, 0.46604478,\n",
              "        0.41779575, 0.505096  , 0.5294779 , 0.42877364, 0.5191441 ,\n",
              "        0.45271164, 0.41333932, 0.42521384, 0.4654701 , 0.41582042,\n",
              "        0.44276556, 0.41458228, 0.5222067 , 0.42307964, 0.52206093],\n",
              "       [0.50649214, 0.47813052, 0.46407682, 0.44625705, 0.4859744 ,\n",
              "        0.48557654, 0.5110846 , 0.44455186, 0.4492663 , 0.46125722,\n",
              "        0.47454688, 0.4793428 , 0.51163507, 0.48368973, 0.5259542 ,\n",
              "        0.44841442, 0.46827224, 0.45150182, 0.46402267, 0.45056236,\n",
              "        0.4521273 , 0.43604803, 0.51118   , 0.45995715, 0.50330675],\n",
              "       [0.48985592, 0.45846924, 0.44760644, 0.4269026 , 0.51097065,\n",
              "        0.534307  , 0.5121933 , 0.44002238, 0.44235095, 0.44538036,\n",
              "        0.52265525, 0.47766548, 0.49547285, 0.53255373, 0.5227472 ,\n",
              "        0.4442276 , 0.5015961 , 0.46812403, 0.444738  , 0.48070288,\n",
              "        0.44638246, 0.43929133, 0.49078   , 0.50282705, 0.5112313 ],\n",
              "       [0.45093197, 0.4706148 , 0.43363348, 0.43096566, 0.5153863 ,\n",
              "        0.518556  , 0.5106921 , 0.48906824, 0.45315275, 0.4863153 ,\n",
              "        0.46994978, 0.507216  , 0.49968696, 0.48243195, 0.4771587 ,\n",
              "        0.48019418, 0.51121134, 0.42873716, 0.47173864, 0.47902447,\n",
              "        0.39598987, 0.4295906 , 0.49048445, 0.48704407, 0.5259362 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#VAEs\n",
        "\n",
        "from tensorflow.keras.layers import Lambda, Dense, Input\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "gWuJeZPmejJX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len_dim = 5\n",
        "encode1 = Dense(15,activation = \"relu\")(input_layer)\n",
        "#mean\n",
        "mean = Dense(len_dim)(encode1)\n",
        "print(mean)\n",
        "#log variance\n",
        "log_var = Dense(len_dim)(encode1)\n",
        "print(log_var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2SZg-VKgkcy",
        "outputId": "5594c3f4-8c17-4f31-caf4-e093039ccd51"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<KerasTensor shape=(None, 5), dtype=float32, sparse=False, name=keras_tensor_12>\n",
            "<KerasTensor shape=(None, 5), dtype=float32, sparse=False, name=keras_tensor_13>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def se(args):\n",
        "    mean,log_var = args\n",
        "    new = K.random_normal(shape=(K.shape(mean)[0],len_dim))\n",
        "    return mean + K.exp(0.5*log_var/2)*new"
      ],
      "metadata": {
        "id": "FjbYD87fidCE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_layer = Lambda(se)([mean,log_var])"
      ],
      "metadata": {
        "id": "-vKtu2StjGWR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode1 = Dense(15,activation = \"relu\")\n",
        "decode_mean = Dense(in_dim,activation = \"sigmoid\")"
      ],
      "metadata": {
        "id": "Q1VAX5ePmFLk"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded1 = decode1(custom_layer)\n",
        "decoded_mean= decode_mean(custom_layer)"
      ],
      "metadata": {
        "id": "IOHIAdLrnII9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#VAE model\n",
        "VAE = Model(input_layer , decoded_mean )"
      ],
      "metadata": {
        "id": "6sDlmCV-n_dM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "loss = (input_layer-decoded_mean)"
      ],
      "metadata": {
        "id": "GkMF3qbxoQ31"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.losses import KLDivergence\n",
        "import tensorflow_probability as tfp"
      ],
      "metadata": {
        "id": "_kfkXi8AscF9"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PABcw5xCpxq_"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JMM7cX2oqRwB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}