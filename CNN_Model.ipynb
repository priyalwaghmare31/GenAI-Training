{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2ae02d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "#load Fashion_MNIST dataset\n",
    "#Normalize the dataset\n",
    "#Build a CNN model using Keras\n",
    "#Train the model\n",
    "#Evaluate the model\n",
    "#Take new images input from users\n",
    "#Preprocess the images\n",
    "#Predict the class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b96319de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Required Libraries --\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.preprocessing import image # for image preprocessing\n",
    "from PIL import Image # for image handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ef0a352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "#Load Fashion_MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24d58df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "#shape\n",
    "print( x_train.shape)\n",
    "print( x_test.shape)\n",
    "print( y_train.shape)\n",
    "print( y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1bc388f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unique value\n",
    "np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4a7dc3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255], dtype=uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50849e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the dataset\n",
    "x_train = x_train / 255.0 #bring pixel values to the range [0, 1] \n",
    "x_test = x_test / 255.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d01a3f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00392157, 0.00784314, 0.01176471, 0.01568627,\n",
       "       0.01960784, 0.02352941, 0.02745098, 0.03137255, 0.03529412,\n",
       "       0.03921569, 0.04313725, 0.04705882, 0.05098039, 0.05490196,\n",
       "       0.05882353, 0.0627451 , 0.06666667, 0.07058824, 0.0745098 ,\n",
       "       0.07843137, 0.08235294, 0.08627451, 0.09019608, 0.09411765,\n",
       "       0.09803922, 0.10196078, 0.10588235, 0.10980392, 0.11372549,\n",
       "       0.11764706, 0.12156863, 0.1254902 , 0.12941176, 0.13333333,\n",
       "       0.1372549 , 0.14117647, 0.14509804, 0.14901961, 0.15294118,\n",
       "       0.15686275, 0.16078431, 0.16470588, 0.16862745, 0.17254902,\n",
       "       0.17647059, 0.18039216, 0.18431373, 0.18823529, 0.19215686,\n",
       "       0.19607843, 0.2       , 0.20392157, 0.20784314, 0.21176471,\n",
       "       0.21568627, 0.21960784, 0.22352941, 0.22745098, 0.23137255,\n",
       "       0.23529412, 0.23921569, 0.24313725, 0.24705882, 0.25098039,\n",
       "       0.25490196, 0.25882353, 0.2627451 , 0.26666667, 0.27058824,\n",
       "       0.2745098 , 0.27843137, 0.28235294, 0.28627451, 0.29019608,\n",
       "       0.29411765, 0.29803922, 0.30196078, 0.30588235, 0.30980392,\n",
       "       0.31372549, 0.31764706, 0.32156863, 0.3254902 , 0.32941176,\n",
       "       0.33333333, 0.3372549 , 0.34117647, 0.34509804, 0.34901961,\n",
       "       0.35294118, 0.35686275, 0.36078431, 0.36470588, 0.36862745,\n",
       "       0.37254902, 0.37647059, 0.38039216, 0.38431373, 0.38823529,\n",
       "       0.39215686, 0.39607843, 0.4       , 0.40392157, 0.40784314,\n",
       "       0.41176471, 0.41568627, 0.41960784, 0.42352941, 0.42745098,\n",
       "       0.43137255, 0.43529412, 0.43921569, 0.44313725, 0.44705882,\n",
       "       0.45098039, 0.45490196, 0.45882353, 0.4627451 , 0.46666667,\n",
       "       0.47058824, 0.4745098 , 0.47843137, 0.48235294, 0.48627451,\n",
       "       0.49019608, 0.49411765, 0.49803922, 0.50196078, 0.50588235,\n",
       "       0.50980392, 0.51372549, 0.51764706, 0.52156863, 0.5254902 ,\n",
       "       0.52941176, 0.53333333, 0.5372549 , 0.54117647, 0.54509804,\n",
       "       0.54901961, 0.55294118, 0.55686275, 0.56078431, 0.56470588,\n",
       "       0.56862745, 0.57254902, 0.57647059, 0.58039216, 0.58431373,\n",
       "       0.58823529, 0.59215686, 0.59607843, 0.6       , 0.60392157,\n",
       "       0.60784314, 0.61176471, 0.61568627, 0.61960784, 0.62352941,\n",
       "       0.62745098, 0.63137255, 0.63529412, 0.63921569, 0.64313725,\n",
       "       0.64705882, 0.65098039, 0.65490196, 0.65882353, 0.6627451 ,\n",
       "       0.66666667, 0.67058824, 0.6745098 , 0.67843137, 0.68235294,\n",
       "       0.68627451, 0.69019608, 0.69411765, 0.69803922, 0.70196078,\n",
       "       0.70588235, 0.70980392, 0.71372549, 0.71764706, 0.72156863,\n",
       "       0.7254902 , 0.72941176, 0.73333333, 0.7372549 , 0.74117647,\n",
       "       0.74509804, 0.74901961, 0.75294118, 0.75686275, 0.76078431,\n",
       "       0.76470588, 0.76862745, 0.77254902, 0.77647059, 0.78039216,\n",
       "       0.78431373, 0.78823529, 0.79215686, 0.79607843, 0.8       ,\n",
       "       0.80392157, 0.80784314, 0.81176471, 0.81568627, 0.81960784,\n",
       "       0.82352941, 0.82745098, 0.83137255, 0.83529412, 0.83921569,\n",
       "       0.84313725, 0.84705882, 0.85098039, 0.85490196, 0.85882353,\n",
       "       0.8627451 , 0.86666667, 0.87058824, 0.8745098 , 0.87843137,\n",
       "       0.88235294, 0.88627451, 0.89019608, 0.89411765, 0.89803922,\n",
       "       0.90196078, 0.90588235, 0.90980392, 0.91372549, 0.91764706,\n",
       "       0.92156863, 0.9254902 , 0.92941176, 0.93333333, 0.9372549 ,\n",
       "       0.94117647, 0.94509804, 0.94901961, 0.95294118, 0.95686275,\n",
       "       0.96078431, 0.96470588, 0.96862745, 0.97254902, 0.97647059,\n",
       "       0.98039216, 0.98431373, 0.98823529, 0.99215686, 0.99607843,\n",
       "       1.        ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be0dcae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape training data\n",
    "x_train = x_train.reshape((-1, 28, 28, 1)) #batch_size, height, width, channels\n",
    "#reshape testing data\n",
    "x_test = x_test.reshape((-1, 28, 28, 1))\n",
    "#batch_size = it is the number of samples processed before the model is updated\n",
    "#channels = it is defined as the number of color channels in the image (1 for grayscale, 3 for RGB)\n",
    "#we take batch_size as -1, which means it will be inferred from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37428985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#create a CNN model\n",
    "#filter : it is the number of output channels in the convolutional layer\n",
    "#kernel_size : it is the size of the filter applied to the input image\n",
    "#activation : it is the activation function applied to the output of the convolutional layer\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32,(3,3), activation='relu', input_shape=(28, 28, 1)), #input_shape is the shape of the input image\n",
    "    layers.MaxPooling2D((2, 2)), #pooling layer to reduce the spatial dimensions\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'), #second convolutional layer\n",
    "    layers.MaxPooling2D((2, 2)), #second pooling layer\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'), #third convolutional layer\n",
    "    layers.Flatten(), #flatten the output of the convolutional layers\n",
    "    layers.Dense(128, activation='relu'), #fully connected layer\n",
    "    layers.Dense(10, activation='softmax') #output layer with 10 classes for Fashion MNIST\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a7d98c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', #optimizer to use for training\n",
    "    loss='sparse_categorical_crossentropy', #loss function for multi-class classification\n",
    "    metrics=['accuracy'] #metrics to evaluate the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "74248dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9892 - loss: 0.0331\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9901 - loss: 0.0287\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9900 - loss: 0.0277\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9909 - loss: 0.0257\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9910 - loss: 0.0270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22039c50fa0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model\n",
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "512c3fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9055 - loss: 0.7631\n",
      "Test Loss: 0.7387503981590271, Test Accuracy: 0.9068999886512756\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0ff12ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take new images from users\n",
    "#output class\n",
    "output = ['Shirt' , 'Pant', 'T-shirt' , 'Trouser' , 'bag' , 'belt' , 'shoes' , 'watch' , 'coat' , 'cap']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dbaf880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take new images input from users\n",
    "def image(images):\n",
    "    a = Image.open(images).convert('L').resize((28, 28)) #convert to grayscale and reshape\n",
    "    a_array = np.array(a)\n",
    "    \n",
    "    a_in = 255 - a_array #invert the pixel values\n",
    "    a_in = a_in / 255.0 #normalize the pixel values\n",
    "    a_in = a_in.reshape((1, 28, 28, 1)) #reshape to match the input shape of the model\n",
    "    pre = model.predict(a_in) #predict the class\n",
    "    pre_class = output[np.argmax(pre)] #get the class name\n",
    "    \n",
    "    plt.imshow(a_in.reshape(28,28), cmap='gray') #display the image\n",
    "    plt.title(pre_class) #display the predicted class\n",
    "    plt.axis('off') #hide the axes\n",
    "    plt.show() #show the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5b3afa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAARD0lEQVR4nO3df6zVdR3H8c/hXu7l4lQUb2VONHWGaBT+QAeYNoZu4IZWm25mba5Ztlyz9Ycz88dmpsw2/ghLC1sIKc452rCpKeZ0SIrNH3MDM6eZDZEGBXkF7uW0c5pvMw3u+6v36/XweGx3Ct4X53i5nCffc7gfGs1ms1kAoJQy5sO+AwCMHqIAQBAFAIIoABBEAYAgCgAEUQAgiAIAQRQACKLAXu+ll14qjUaj3HjjjXt836uvvrr9vtCpRIFRr/UgPJy33//+92U0eeONN9oRGW33C3ane7f/FUaB22677R3fXrJkSfnd7373ru8/5phjRvy+XHHFFeWyyy4bdhSuueaa9r+ffvrpI3zP4IMhCox6X/nKV97x7TVr1rSj8L/fX4fu7u722+7s2rWr7Nixo7b7BB8kTx/R8dauXVvOPPPMctBBB5W+vr7yqU99qlx44YXv+b633HJLOfLII0tvb2856aSTyhNPPLHH1xRa3/72t79dli1bVo499tj29mc/+1np7+9v//fW1cJbT3G19jCauVKgo23cuLGcccYZ7Qfo1tM+EyZMaL+wfPfdd7/rfX/961+XrVu3lm984xvtB/AFCxaUL37xi+XFF18sY8eO3e3trFq1qtx5553tOLTi89nPfrb89Kc/LRdffHE555xz2j9Oy9SpU0fs/xU+CKJAR1u9enXZvHlzuf/++8uJJ54Y33/ttde+633/8pe/lD/96U/lgAMOaH/705/+dJk/f3657777yllnnbXb21m/fn159tlny5QpU+L7WvtWFFoh+DCe6oIqPH1ER2tdGbSsXLmy7Ny5c7fve+6550YQWk499dT2P1tXCnty2mmnvSMI8FElCnSEbdu2lQ0bNsTb66+/Hg/WX/rSl9rP67ee1mn9zv+Xv/xl2b59+7t+jEmTJr3j228FonWlsSet1ymgE4gCHaH1hWcHH3xwvLVeJG5pvTZw1113lccee6z9fP+rr77afpH5hBNOaIfkv3V1db3njz2cv7G29QI2dAKvKdARvvrVr5ZZs2b93wfpU045pf32wx/+sP2C8vnnn1/uuOOO8vWvf33E7pOvfOajSBToCEcccUT77X+1nvppva7w3w/Qn/vc59r/fK+nkD5I48ePb/9zy5YtI3o78EESBTrar371q3LTTTe1/1ho6+sPWn/k9Oc//3nZb7/9yty5c0f0tltXK60Xn5cvX16OPvrocuCBB5bjjjuu/QajlSjQ0VovND/++OPtp4pee+21sv/++5fp06e3v9CsjheHf/GLX5RLLrmkXHrppe2vcr7qqqtEgVGt0RzOq2gA7BX86SMAgigAEEQBgCAKAARRACCIAgD5r1PwJfuda5999klvWn/uP2vmzJmlij39XQbvpa4/af3/zkvanapfSf2/ZzUNx3XXXZfetP6CIDrTcH5duFIAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAAiiAED+72h2IF7nuuaaa9KbefPmpTd33nlnqWLJkiXpzeDgYHpT5XO8p6cnvfn85z9fqpg2bVp6c8EFF6Q3kydPTm/+8Y9/pDfUz4F4AKSIAgBBFAAIogBAEAUAgigAEEQBgCAKAARRACCIAgBBFAAIogBA6H77XxlNDj/88Eq7Koetfe1rX0tvzj777PTmiiuuKFWsWrUqvRkYGKjlEL0333wzvZk+fXqpYsWKFenN7Nmz05u5c+emN7fffnt6w+jkSgGAIAoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAAiNZrPZLMPQaDRKp5k0aVJ6c/HFF6c3c+bMSW8mTJhQqti0aVN6s379+lpOVl2wYEGp6+fpkUceSW+6urrSm0MOOSS9OfXUU0sVM2fOTG8WLVqU3sybN6+Wz7sqp9+2rFy5Mr1ZvXp1erNz587SaYbzcO9KAYAgCgAEUQAgiAIAQRQACKIAQBAFAIIoABBEAYAgCgAEUQAgiAIAobt0iIkTJ9ZyaNpzzz2X3ixevDi9GRoaKlWMGzeulsP3urvznzrLli0rVVx55ZW1HARXxU033ZTe3HrrrZVua5hnV77Dxo0b05vrr78+vent7U1vTjjhhFLF7bffnt68+uqr6c0ZZ5yR3mzevLl81LlSACCIAgBBFAAIogBAEAUAgigAEEQBgCAKAARRACCIAgBBFAAIogBA5x2Id8MNN6Q3TzzxRHrz2GOPpTc7d+5Mb1544YVSxVNPPZXeHHHEEenNxz72sfTmsMMOK1UceeSR6c38+fPTm76+vvTmM5/5TG0HAx566KHpzcMPP5zeDAwMpDdTp05Nbx588MFSxQMPPJDeLFiwoJaDGC+99NLyUedKAYAgCgAEUQAgiAIAQRQACKIAQBAFAIIoABBEAYAgCgAEUQAgiAIAodFsNptlGBqNRqnDaaedVml36623pjfTpk1Lb2bPnl3LAV5bt24tdalyuN3MmTNrOwhu27Zt6U1XV1d6U+VzvMqBc2effXapospha0uXLk1vNm3aVOpw1FFHVdrNmzevlp+nhx56KL05+eSTSxXPP/98qcNwHu5dKQAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAoJ4D8Xp7e9ObNWvWlCp+/OMf13JY2JQpU9Kbvr6+9ObJJ58sdanyc7t48eL0pr+/v1SxaNGi9Oa6665Lb5577rlafp5OOeWUUsX48ePTm/nz56c3w3xIeN8mT55c6rJu3br0Zvny5bUcxNjy5S9/udTBgXgApIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQBCdxlB3/3ud9Ob119/vdJtLVu2rNThr3/9a3ozY8aMMppVORXzt7/9bXrz8MMPlyq2bt2a3vzhD39Ib1566aX0ZuHChelNT09PqWLq1Kmj9sTTKg477LBKu0ceeaTUYWmFU5RXrFhR6bZOPvnkWj7Hh8OVAgBBFAAIogBAEAUAgigAEEQBgCAKAARRACCIAgBBFAAIogBAEAUA8gfiHXfccSXrW9/6VnozZ86cUkVdB39t27YtvRk7dmzpNHfddVdtt/XJT34yvXn++edH7UFrO3bsqLRbu3ZtGa26u/NnazYajUq39cYbb5Q6vPzyy+nNfffdV+m2fvSjH6U3s2fPLiPBlQIAQRQACKIAQBAFAIIoABBEAYAgCgAEUQAgiAIAQRQACKIAQBAFAMKwT7G6+eabS9Ytt9yS3qxbt66MZrt27UpvJk6cmN50dXWVKoaGhkqnqXLY2qZNmzrqwLnRrq+vb9QeYlnVK6+8kt5cfvnltR0wed5555WR4EoBgCAKAARRACCIAgBBFAAIogBAEAUAgigAEEQBgCAKAARRACCIAgBh2CeNTZkypWRddNFFtRwe1/L3v/+91GH//fdPb2bNmpXeLFmyJL3pVGPGjKnl4EKqGzt2bHozODhYRrPNmzenN+PGjat0WzfeeGMtm+FwpQBAEAUAgigAEEQBgCAKAARRACCIAgBBFAAIogBAEAUAgigAEEQBgCAKAORPSR0YGChZ8+bNS2/uv//+UkV/f396s27duvRm7ty56c20adPSG6d8vr9TUnt6etKbT3ziE+nNhg0b0huq/1pqWbVqVXrTbDbTm0ajkd6cdNJJpYq1a9emN93dw374TnGlAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGA0GgO86SoOXPmlKylS5emN3fccUepYuHChelNb29venPvvfemN5s3b05vjj/++PSmUx1++OHpzUUXXZTeDA4OpjdXXXVVLYezjXb77LNPevPiiy9Wuq1vfvOb6c2KFSvSm2OPPba2gyx/8IMf1PL4dc455+zxfVwpABBEAYAgCgAEUQAgiAIAQRQACKIAQBAFAIIoABBEAYAgCgAEUQAgdJdheuCBB0rWrFmz0pvly5eXKiZPnpzebNmyJb158803azkQj7eNGzcuvXn22WdrOTStEw+3q0vVj93NN99cy2099dRT6c3ixYtLFRs3bkxvLrjggvRm27Zte3wfVwoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAMgfiFfFCy+8UMshei2LFi1Kb04//fT05jvf+U56873vfS+94W3d3flP0x07dqQ3AwMD6Q3/0dXVld68/PLLlW7r6aefTm8WLlyY3vzrX/9Kb37zm9+UKi6//PIyWrhSACCIAgBBFAAIogBAEAUAgigAEEQBgCAKAARRACCIAgBBFAAIogBAPQfiVVH1ULILL7wwvZkwYUJ6c9RRR9VyoBtvGzMm/3uXoaGhEbkvfHB6enoq7b7//e+nN9u3b09v1q9fn9785Cc/KR91rhQACKIAQBAFAIIoABBEAYAgCgAEUQAgiAIAQRQACKIAQBAFAIIoABBEAYCwVx/fuWXLlvSmt7c3vXFiZ/0OOuig9Gb69OnpzeOPP57e8B9dXV2VdlVOHb7kkksq3dbeyJUCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQDCXn0gXhWNRqOWDe9Pf39/ejNjxoz0xoF41VX9dTFmjN/LjiQfXQCCKAAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABAfi1XAYV7PZHJH7srfo6elJb5555pn05p577klvqJ8D8UaWjy4AQRQACKIAQBAFAIIoABBEAYAgCgAEUQAgiAIAQRQACKIAQBAFAIID8ZIciAcfrkaj8WHfhY7mSgGAIAoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAMGBeDUcxuVAvPoPIRwcHByR+8KH/znuQLyR5UoBgCAKAARRACCIAgBBFAAIogBAEAUAgigAEEQBgCAKAARRACCIAgBBFAAITklN6urqSm+ckvr+7Nq1K735+Mc/nt7MmDEjvVm9enV604mckto5XCkAEEQBgCAKAARRACCIAgBBFAAIogBAEAUAgigAEEQBgCAKAARRACA4EK+GA/GqHOjG+9Pf35/eTJw4Mb1xIF79B+KNGeP3siPJRxeAIAoABFEAIIgCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAMGBeEnd3fkP2dDQ0Ijcl71FlY/5H//4x/TmoYceSm+o/jle9aBIB+KNLB9dAIIoABBEAYAgCgAEUQAgiAIAQRQACKIAQBAFAIIoABBEAYAgCgAEB+LVcDjbfvvtl940Go1SRbPZLJ1mcHCwtsPWqKbKx3v8+PG1/Rpk+FwpABBEAYAgCgAEUQAgiAIAQRQACKIAQBAFAIIoABBEAYAgCgAEUQAgiAIAwXGDSWvWrElv/vnPf6Y3jz76aKni6aefTm+eeeaZ9ObPf/5zevO3v/2tVHHooYemN2PG5H+/c/DBB6c3PT096U1fX1+p4sADD0xvJk2alN5MnTo1vZkxY0Z688orr5QqXnvttUo7hseVAgBBFAAIogBAEAUAgigAEEQBgCAKAARRACCIAgBBFAAIogBAEAUAQqPZbDbLMDQajeG8G++huzt/7uAXvvCFSrd15plnpjcnnnhiLYfU7bvvvqWK3t7eWj7mQ0NDpQ6Dg4OVdgMDA+nNhg0bajlU8Z577klvVq5cWarYvn17pR2lDOfh3pUCAEEUAAiiAEAQBQCCKAAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQDyB+IB0PlcKQAQRAGAIAoABFEAIIgCAEEUAAiiAEAQBQCCKABQ3vJvt+SFWrYmp+sAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image (\"C:\\\\Users\\\\hp\\\\Desktop\\\\Gen AI\\\\shirt.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
